{"cells":[{"cell_type":"markdown","source":["# SI 618 - Homework 9 - Classification of Employee Attrition\n\n## Objectives\n* Be able to extract, transform, and select multiple features to prepare for classification algorithm\n* Be able to apply random forest regression\n* Be able to evaluate classification results using appropriate metrics\n\n## Submission Instructions:\nPlease submit your completed Databricks notebook file in .html format as well as the URL to the published version of your notebook via Canvas."],"metadata":{}},{"cell_type":"markdown","source":["## Goal: \n1. Try to predict the IBM employee attrition using multiple factors, and evaluate how good your predictions are; \n2. Find out the leading drivers of Employee Attrition.\n\nThe dataset is downloaded from IBM's website:\nhttps://www.ibm.com/communities/analytics/watson-analytics-blog/guide-to-sample-datasets/\n\n#### NOTE: This homework assignment follows very closely the structure of this week's lab assignment.\n\nYou should be able to complete the core (i.e. everything other than \"Above and Beyond\") of this\nassignment based on the code in the lab notebook."],"metadata":{}},{"cell_type":"markdown","source":["#### Read the dataset from AWS S3 bucket."],"metadata":{}},{"cell_type":"code","source":["ACCESS_KEY = \nSECRET_KEY = \nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"umsi-data-science-west\"\nMOUNT_NAME = \"umsi-data-science\"\ntry:\n  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\nexcept:\n  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\ndisplay(dbutils.fs.ls(\"/mnt/umsi-data-science/si618wn2017\"))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Read the data (in AWS as a csv) in a pyspark DataFrame\nibm = spark.read.csv(\"/mnt/umsi-data-science/si618wn2017/WA_Fn-UseC_-HR-Employee-Attrition.csv\", inferSchema=True, header=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["string_columns = [\"Attrition\",\"BusinessTravel\",\"Department\",\"EducationField\",\"Gender\",\"JobRole\",\"MaritalStatus\",\"Over18\",\"OverTime\"]\ncolumns = ibm.columns\n\n# Anna's trick to create new columns\nfor x, y in enumerate(columns):\n  if y in string_columns:\n    columns[x] = \"indexed_\"+y\n\n# Step 1:\nibm_string_indexer = [StringIndexer(inputCol=c, outputCol=\"indexed_\"+ c) for c in string_columns]\n\n# remove target column (now called \"indexed_attrition\")\ncolumns.remove(\"indexed_Attrition\")\n\n# Step 2:\nibm_assembler = VectorAssembler(\n    inputCols=columns,\n    outputCol=\"features\")\n\n# Step 3:\nibm_cat_indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexed\", maxCategories=3)\n\n# Step 4:\nibm_rf = RandomForestClassifier(featuresCol=\"indexed\", labelCol=\"indexed_Attrition\", numTrees=10)\n\n# Step 5: (Before this wasn't working because I was trying to fit a list)\nibm_labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", \n\tlabels=ibm_string_indexer[0].fit(ibm).labels)\n\n# Step 6:\nsplits = ibm.randomSplit([0.8, 0.2], 1234)\nibm_train = splits[0]\nibm_test = splits[1]\n\n# Step 7:\nibm_pipeline = Pipeline(stages=ibm_string_indexer + [ibm_assembler, ibm_cat_indexer, ibm_rf, ibm_labelConverter])\n\n# Step 8:\nibm_model = ibm_pipeline.fit(ibm_train)\nibm_predictions = ibm_model.transform(ibm_test)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Step 9 (what happened = \"attrition\"; our prediction = \"prediction\")\ndisplay(ibm_predictions.select(\"Attrition\",\"predictedLabel\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Step 10: (we compare indexed variables (i.e., indexed_Attrition and prediction) not (Attrition and predictedLabel))\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Attrition\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(ibm_predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Step 11:\nimport pandas as pd\nrf_features = pd.DataFrame({\"index\":ibm.columns[:-1],\"featureImportances\":ibm_model.stages[-2].featureImportances})\\\n  .sort_values(\"featureImportances\", ascending=False)\nrf_features['Rank'] = rf_features['featureImportances'].rank(ascending=0)\n# nicer table if you make index the pandas DataFrame index\nindexed_rf_features = rf_features.set_index(['index'])\nindexed_rf_features.head(10)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Step 12:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(12, 7))\nsns.set(style=\"darkgrid\")\nsns.barplot(y=\"index\",x=\"featureImportances\",data=rf_features)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(f.figure)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["What do we see?\n- MaritalStatus is the most important predictor of attrition. Of course, marital status can be a proxy for whether or not an employee becomes a parent, which could lead an employee to leave. This suggests IBM should re-consider (and improve) their policies for parents. It could also be a strong predictor because employees that are unmarried are also younger, and more likely to leave. So we'd want to know the direction of the feature, but Random Forrest Classifiers don't offer this info (they are multi-dimensional).\n- Pay matters. Stock Options and Years Since Last Promotion (another proxy) are predictive.\n- Job level and department also matter. This could suggest some departments are more likely to have people leave, because of the nature of the work (e.g., sales people may leave more often) and some roles may be more likely to have attrition (e.g., analysts, who spend two or three years at IBM and then go to business schools for an MBA)."],"metadata":{}},{"cell_type":"markdown","source":["## Above and Beyond\n\n2. Repeat the analysis for Steps 4-12 using Gradient-Boosted Trees (https://spark.apache.org/docs/latest/ml-classification-regression.html#gradient-boosted-tree-classifier) and compare the results to Random Forest. Describe, in detail, how the classification results differ. The majority of your work should go into exploring the differences in the results."],"metadata":{}},{"cell_type":"code","source":["# Gradient-Boosted Trees\nfrom pyspark.ml.classification import GBTClassifier\n\n# Re-do Step 4:\nibm_gbt = GBTClassifier(featuresCol=\"indexed\", labelCol=\"indexed_Attrition\", maxIter=10)\n\n# Re-run Step 6:\nsplits = ibm.randomSplit([0.8, 0.2], 1234)\nibm_train = splits[0]\nibm_test = splits[1]\n\n# Step 7:\nibm_pipeline_gbt = Pipeline(stages=ibm_string_indexer + [ibm_assembler, ibm_cat_indexer, ibm_gbt, ibm_labelConverter])\n\n# Step 8:\nibm_model_gbt = ibm_pipeline_gbt.fit(ibm_train)\nibm_predictions_gbt = ibm_model.transform(ibm_test)\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_Attrition\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(ibm_predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Now calculate feature importance for GBT model\ngbt_features = pd.DataFrame({\"index\":ibm.columns[:-1],\"featureImportances\":ibm_model_gbt.stages[-2].featureImportances})\\\n  .sort_values(\"featureImportances\", ascending=False)\ngbt_features['Rank'] = gbt_features['featureImportances'].rank(ascending=0)\n# nicer table if you make index the pandas DataFrame index\nindexed_gbt_features = gbt_features.set_index(['index'])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# add column to each dataframe, to make getting it into Seaborn format a bit easier\ngbt_features['type'] = \"gbt\"\nrf_features['type'] = \"rf\"\n\nall_features = pd.concat([rf_features, gbt_features])\n# This code isn't displaying because DataBricks is a pain with graphing :(\n#g = sns.factorplot(x=\"index\", y=\"featureImportances\", hue=\"type\", data=all_features,\n                   size=6, kind=\"bar\", palette=\"muted\")\n#g.despine(left=True)\n#g.set_ylabels(\"\")\n#display(g.FacetGrid)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["all_features.head()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["indexed_gbt_features.head(20)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Our RF model indicated that Marital status was the strongest predictor (in terms feature importance), and the second strongest was Over 18. Over 18 was strange because all of the employees are over 18 years old, so there is no variation in the data (going to Kaggle confirmed this). So why is it such a strong predictor?\n\nThis brings us to the question why do the features have different relative importances?\n- A Random Forrest Classifier uses a bagging ensemble to derive predictors for each feature--it bootstraps samples, stores the feature importances, and then averages them. The key idea is that each observation in the bootstrap has the same probability of appearing in each boot.\n- A Gradient Boosted Classifier uses a boosting ensemble technique, which in simple terms gives more weight to some observations when calculating feature importances. A blog post on Medium explains: \"Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most... new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions\"\n\nIn one sentence, the difference between RF and GBT is that the former is parallel, and the later is sequential. \n\nWhy does GBT throw out Over18 feature, which has no variation? The same Medium article says \"the intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better.\" The Over18 feature has no patterns in the residuals -- zero variation, zero residuals. So the feature isn't as important.\n\n\nFYI, here are the links to the articles that I read to learn the difference between RF and GBT:\n- http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n- https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"],"metadata":{}},{"cell_type":"markdown","source":["## End of Homework 9"],"metadata":{}}],"metadata":{"name":"Jacozart Homework 9","notebookId":4246223641695640},"nbformat":4,"nbformat_minor":0}
