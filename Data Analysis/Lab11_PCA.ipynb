{"cells":[{"cell_type":"code","source":["ACCESS_KEY = \nSECRET_KEY = \nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"si618dimension\"\nMOUNT_NAME = \"si618dimension\"\n\n# Comment the following line if you need to unmount the S3 bucket\n# dbutils.fs.unmount(\"/mnt/si618dimension/\")\n\n# mount the S3 bucket\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/si618dimension/\"))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["heptathlon = spark.read.csv(\"/mnt/si618dimension/heptathlon.csv\", inferSchema=True, header=True)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(heptathlon)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.stat import Correlation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import col"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## 1. Perform Principal Component Analysis (PCA)"],"metadata":{}},{"cell_type":"markdown","source":["### 1.1 Assemble the Features"],"metadata":{}},{"cell_type":"code","source":["# list comprehension to include only the variables that are neither score nor athlete\nfeature_columns = [column for column in heptathlon.columns if column != 'Score' and column != 'Athlete']\n\n# now concat into one vector\nassembler = VectorAssembler(\n    inputCols=feature_columns,\n    outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### 1.2 Rescale the Data\nLet us normalize the data by computing the \"[z-scores](https://en.wikipedia.org/wiki/Standard_score#Calculation_from_raw_score)\" of the values. This step of normalization (rescaling) is important to PCA.\n\nGiven a column x, the z-scores of the elements in x is simply\n\n    (x - mean(x)) / std(x)\n    \nwhere, `std` means standard deviation."],"metadata":{}},{"cell_type":"code","source":["# pre-process normalization\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=True, withMean=True)\n#withMean=True suggests mean is subtracted"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### 1.3 Perform Principal Component Analysis (PCA)"],"metadata":{}},{"cell_type":"code","source":["# k can't be higher than the number of variables. Max N of axis can be at most the number of original features.\npca = PCA(k=7, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### 1.4 Fit the Pipeline and Transform the Data"],"metadata":{}},{"cell_type":"code","source":["# hard to debug pipeline. Solution: run pipeline with the first function. If it works, add the second. Then the third.\npipeline = Pipeline(stages=[assembler, scaler, pca])\npipelineModel = pipeline.fit(heptathlon)\nheptathlon_transformed = pipelineModel.transform(heptathlon)\n# heptathlon_transformed has three new columns: assembled vector, scaled vectored, then scaled pca values."],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### 1.5 Choose the optimal number of Principal Components (PC) to retain"],"metadata":{}},{"cell_type":"code","source":["pcaModel = pipelineModel.stages[-1]\npcaModel.explainedVariance\n# this gives the percents that each pca value accounts for the explained variation of the factor"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["f, ax = plt.subplots(figsize=(5, 3))\nplt.plot(range(1,8), pcaModel.explainedVariance, 'b-o')\ndisplay(f.figure)\n# select three principle components for later analysis"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## 2. Comparison of Original Features and Principal Components"],"metadata":{}},{"cell_type":"markdown","source":["### 2.1 Create a Correlation Matrix"],"metadata":{}},{"cell_type":"markdown","source":["#### Using Original Features"],"metadata":{}},{"cell_type":"code","source":["r1 = Correlation.corr(heptathlon_transformed, \"features\", \"pearson\").head()\nprint(\"Pearson correlation matrix:\\n\")\noriginal_correlation = pd.DataFrame(r1[0].toArray(), columns=feature_columns)\noriginal_correlation"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["#### Using Principal Components"],"metadata":{}},{"cell_type":"code","source":["r2 = Correlation.corr(heptathlon_transformed, \"pcaFeatures\", \"pearson\").head()\nprint(\"Pearson correlation matrix:\\n\")\nPC_correlation = pd.DataFrame(r2[0].toArray(), columns=[str(\"PC\"+str(i)) for i in range(1,8)])\nPC_correlation"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### 2.2 Draw a \"clustered\" heatmap\nRefer to: https://seaborn.pydata.org/generated/seaborn.heatmap.html"],"metadata":{}},{"cell_type":"markdown","source":["#### Using Original Features"],"metadata":{}},{"cell_type":"code","source":["f, ax = plt.subplots()\nsns.clustermap(original_correlation,figsize=(8,5))\ndisplay(f.figure)\n#hierarchical clustering. Not using bisecting k means, but instead hierarchical clustering. Put two most correlated into one cluster, then add one more."],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["#### Using Principal Components"],"metadata":{}},{"cell_type":"code","source":["f, ax = plt.subplots()\nsns.clustermap(PC_correlation,figsize=(8,5))\ndisplay(f.figure)\n# The colors only show on diagonal -- principal components are orthogonal to eachother in geometric space. Linearly uncorrelated."],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### 2.3 Predict Heptathlon Score and Evaluate Models using Cross Validation"],"metadata":{}},{"cell_type":"markdown","source":["#### Linear Regression Using Original Features"],"metadata":{}},{"cell_type":"code","source":["lr = LinearRegression(featuresCol=\"features\",labelCol=\"Score\")\nparamGrid = ParamGridBuilder().build()\nevaluator = RegressionEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"rmse\")\n\n# can use cross val for two things: minimize errors, but then also tune the number of trees, depth, etc.\n# here not interested in finding best parameters\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\ncrossvalModel = crossval.fit(heptathlon_transformed)\ncrossvalModel.avgMetrics"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["#### Principal Component Regression Using All PCs"],"metadata":{}},{"cell_type":"code","source":["lr = LinearRegression(featuresCol=\"pcaFeatures\",labelCol=\"Score\")\nparamGrid = ParamGridBuilder().build()\nevaluator = RegressionEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"rmse\")\n\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\ncrossvalModel = crossval.fit(heptathlon_transformed)\ncrossvalModel.avgMetrics\n# similar result since we don't discard any information"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["#### Principal Component Regression Using 3 PCs"],"metadata":{}},{"cell_type":"code","source":["heptathlon_transformed.printSchema()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["k = 3\nselect_first_k_PC = udf(lambda x: Vectors.dense([c for c in x[:k]]), VectorUDT())\nheptathlon_transformed = heptathlon_transformed.withColumn(\"first_k_pcaFeatures\", select_first_k_PC(col(\"features\")))\ndisplay(heptathlon_transformed.select(\"first_k_pcaFeatures\"))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["lr = LinearRegression(featuresCol=\"first_k_pcaFeatures\",labelCol=\"Score\")\nparamGrid = ParamGridBuilder().build()\nevaluator = RegressionEvaluator(labelCol=\"Score\", predictionCol=\"prediction\", metricName=\"rmse\")\n\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\ncrossvalModel = crossval.fit(heptathlon_transformed)\ncrossvalModel.avgMetrics\n# error is much much larger. When we do PCA, we do something on X, but not on Y. Possible that some variation with Y is discarded."],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["## End of Lab 11\n## Nothing needs to be submitted"],"metadata":{}}],"metadata":{"name":"Lab11 (1)","notebookId":350140759138945},"nbformat":4,"nbformat_minor":0}
