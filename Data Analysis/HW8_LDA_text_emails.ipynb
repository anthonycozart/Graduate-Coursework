{"cells":[{"cell_type":"markdown","source":["# SI 618 Homework 8 - Improving LDA\n\n## Objectives\n* to gain practical experience with NLP techniques\n* to be exposed to loading large datasets via AWS S3 and parquet format\n\n## Submission Instructions:\nPlease turn in your completed Databricks notebook in HTML format as well as the URL to the published version of your completed notebook.\n\n## Assignment Instructions:\nIn this week's lab, we investigated the use of latent Dirichilet allocation (LDA) to analyze text.\nIn particular, we applied LDA to the Enron Corp. email data.  In this homework assignment we\nare going to ask you to revisit the Enron LDA analysis and try to improve it.  \n\nRecall that LDA seeks to extract a number of topics (the number is supplied by you) from a \ncollection of documents. Each one of those topics can be described by the words that are most closely\nassociated with it and thereby facilitate the interpretation of the topic.  \nFor example, a topic that is most closely associated with the words blue, green, \nyellow, red, and purple might be interpreted as being about \"colors\".  That's an ideal\nexample.  In practice, the words that are associated with the topics often don't lead us to \nan easy interpretation of the topic.  In some cases, we can improve the interpretability of\nthe topics.  \n\nFor example, we can manipulate the model parameters (e.g. changing the number of topics) or\nwe can try to do a better job of cleaning the data before analyzing it.  We can experiment\nwith the inclusion or exclusion of stopwords.  Or we can get very creative and use bigrams or\ntrigrams instead of unigrams (words) in our analysis.\n\nThis homework assignment provides you with an oppportunity to improve the LDA we performed on\nthe Enron data, which is reproduced below.  To start this lab, run the cells below and examine the\noutput. Describe the topics and comment on the quality and/or interpretability of the topics. Then, follow the steps below to apply some of the techniques mentioned above.\n \nOne measure of the \"goodness\" of a topic model is the interpretability of the topics.  That is,\ndo the words associated with the topic form a coherent set (like the colors example above) or\nare the seemingly random words?\n\nYou will notice that we're using a 1% sample of the email corpus (note the ```sample(0.01)``` function). Another measure of the \"goodness\" of a topic model is the stability of the model over different random samples.  \nWhat happens to your topics when you re-run the analysis (thereby sampling a differnt 1%).  What happens when you\nrun your analysis on the complete email corpus?\n\nThere are also two numerical measures of model goodness that are available:  log(perplexity) and log(likelihood).\nLower values of log(perplexity) are better, whereas higher values of log(likelihood) are generally\nconsidered better.   You can use these \"objective\" measures in combination with the \"subjective\" assessments of \nthe interpretability of topics when assessing your model.\n\nThis assignment is worth a total of 80 points.  You will receive up to 16 points for each of the following 4 improvements:\n1. Vary the number of topics from 6 to 12 (i.e. 6, 7, 8, 9, 10, 11, and 12).  Which value(s) gives you the \"best\" solution?  What criteria did you use for determining how good each solution is? \n2. How does the topic model change if you include or exclude stopwords? What's the best way to deal with non-alpha chacaters (e.g. numbers)? Is it better to include or exclude stopwords?  Use the \"better\" version in subsequent steps.\n3. Clean the text from the body of each email message by excluding the \"quoted replies\" (i.e. the copy of the original message\nthat is often included in a reply).  How do the results of your topic model change? (Note: you might want to use RDDs and regular expressions for part of this analysis.)\n4. Given the model from the \"best\" number of topics from Step 1, the best choice of including or excluding stopwords, \nand using cleaned email bodies, how consistent/stable are the topics from \nmultiple runs (i.e. using different 1% samples)?  How do you define consistency and stability?\n\nFor each improvement, you will be assessed on:\n\n1. the clarity of your code (both in terms of programming aspects such as variable names and in terms of Markdown cells explaining what you did),\n2. the completeness of your interpretations, and\n3. the quality of the presentation of your results (e.g. using tables and/or visualizations as appropriate).\n\n### Above and Beyond\nSelect one of the following options for up to 16 points:\n1. Use LDA to create two additional topic models based on (1) bigrams and (2) trigrams.  Find the best number of topics, determine whether to include\nstopwords, and clean the email bodies.  How do these topics compare with the ones from the unigram analysis above in terms of interpretability and stability?\n2. In the cell below, the LDA model of the enron DataFrame is stored in a DataFrame called ```enron_lda```.  If you examine that DataFrame you will notice a column called ```topicDistribution```, which tells you the proportion of each topic that makes up each document.  For each document (i.e. row in ```enron_lda```), figure out which topic is the dominant one and label that document as belonging to that topic.  So, for example, if you \nhave a 6-topic model and see\n```\ntopicDistribution=DenseVector([0.011, 0.0114, 0.9446, 0.0111, 0.011, 0.0109]\n```\nfor a document, you would label that document as topic \"3\" because the largest number (0.9446) is associated with topic #3.  Based on this approach, report the number of documents that are labelled with each topic number.\n\n### End of instructions... code follows"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, RegexTokenizer \nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml.pipeline import Pipeline\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords as nltkstopwords\nnltk.download(\"book\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["ACCESS_KEY = \nSECRET_KEY = \nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"umsi-data-science-west\"\nMOUNT_NAME = \"umsi-data-science\"\ntry:\n  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\nexcept:\n  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n#display(dbutils.fs.ls(\"/mnt/umsi-data-science/si618wn2017\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# This is a helper function that looks up the words associated with indices.  \n# It's used below.\nfrom pyspark.sql.types import ArrayType, StringType\n\ndef indices_to_terms(vocabulary):\n    def indices_to_terms(xs):\n        return [vocabulary[int(x)] for x in xs]\n    return udf(indices_to_terms, ArrayType(StringType()))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# The next line loads the Enron email dataset from parquet format.  For details, see\n# https://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files\n# Note the following line takes a sample of approximately 1% of the rows\nenron = spark.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").sample(False,0.00001)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# This cell is a complete machine learning pipeline to run LDA on a dataset\n# Note that you might want to split this up into individual cells for\n# your assignment.  \n\nk = 6 # set the number of topics to extract\n\ntokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n\nstopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nstopWordsRemover.loadDefaultStopWords(\"english\")\n\n#minDF=2 means word has to occur at least twice\nvectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=2) \n\nprint (\"k = \",k)\nlda = LDA(k=6, maxIter=10)\n\n# we've defined all of the transformers and estimators in our pipeline. Now set up \npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\npipelineModel = pipeline.fit(enron)\n\ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\nprint(\"Vocab length is\",len(cmv))\n\nldaModel = pipelineModel.stages[-1]\n\n# Assess the model using .transform()\nenron_lda = pipelineModel.transform(enron)\n\nlp = ldaModel.logPerplexity(enron_lda)\nprint(\"Log perplexity  (lower is better): \",lp)\nll = ldaModel.logLikelihood(enron_lda)\nprint(\"Log likelihood (higher is better): \",ll)\n\n# Describe topics.\ntopics = ldaModel.describeTopics(8)\ntopics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\ntopics.select(\"topicWords\").show(10,truncate=False)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["##1. Determine the optimal number of topics."],"metadata":{}},{"cell_type":"code","source":["k = []\nlog_perplexity = []\nlog_likelihood = []\n\nfor kay in range(6,13,1):\n  print (\"k = \",kay)\n  lda = LDA(k=kay, maxIter=10)\n  pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\n  pipelineModel = pipeline.fit(enron)\n  \n  countVectorModel = pipelineModel.stages[-2]\n  cmv = countVectorModel.vocabulary\n\n  ldaModel = pipelineModel.stages[-1]\n  enron_lda = pipelineModel.transform(enron)\n  \n  k.append(kay)\n  lp = ldaModel.logPerplexity(enron_lda)\n  lp = round(lp, 4)\n  log_perplexity.append(lp)\n  ll = ldaModel.logLikelihood(enron_lda)\n  ll = round(ll, 4)\n  log_likelihood.append(ll)\n\ndf_lda_scores = pd.DataFrame({'perplexity': log_perplexity, 'likelihood': log_likelihood}, index=k)\ndf_lda_scores.sort_values(by=['likelihood','perplexity'], ascending=[False,True])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["After running LDA analyses with a range of topics (at least 6, at most 12), we conclude that the 6 topic model has the highest likelihood and lowest perplexity. Let's use this model below:"],"metadata":{}},{"cell_type":"code","source":["lda = LDA(k=6, maxIter=10)\npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\npipelineModel = pipeline.fit(enron)\n\ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\n\nldaModel = pipelineModel.stages[-1]\nenron_lda = pipelineModel.transform(enron)\n\nlp_no_stop = ldaModel.logPerplexity(enron_lda)\nprint(\"Log perplexity  (no stopwords): \",lp_no_stop)\nll_no_stop = ldaModel.logLikelihood(enron_lda)\nprint(\"Log likelihood (no stopwords): \",ll_no_stop)\n\ntopics = ldaModel.describeTopics(6)\ntopics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\ntopics.select(\"topicWords\").show(6,truncate=False)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["##2. Now tweak pipeline, building it without a stop words remover and by cleaning the text of numbers."],"metadata":{}},{"cell_type":"code","source":["# Re-run analysis without a stop words remover in our pipeline\ntokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\nvectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"features\", minDF=2)\nlda = LDA(k=6, maxIter=10)\npipeline = Pipeline(stages=[tokenizer, vectorizer, lda])\npipelineModel = pipeline.fit(enron)\n\ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\n\nldaModel = pipelineModel.stages[-1]\nenron_lda = pipelineModel.transform(enron)\n\nlp_w_stop = ldaModel.logPerplexity(enron_lda)\nprint(\"Log perplexity  (w/ stopwords): \",lp_w_stop)\nll_w_stop = ldaModel.logLikelihood(enron_lda)\nprint(\"Log likelihood (w/ stopwords): \",ll_w_stop)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["topics = ldaModel.describeTopics(6)\ntopics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\ntopics.select(\"topicWords\").show(6,truncate=False)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["It's clear that stop words shouldn't be included. Many of the topics include stop words, which are not that informative. In fact one of our topics is only stop words (\"of, the, to, and, you). Below, we can re-run our pipeline after removing numerical characters from the text:"],"metadata":{}},{"cell_type":"code","source":["tokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"[a-zA-Z]*\", gaps=False)\nstopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nstopWordsRemover.loadDefaultStopWords(\"english\")\nvectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=2)\nlda = LDA(k=6, maxIter=10)\npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\npipelineModel = pipeline.fit(enron)\n\ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\n\nldaModel = pipelineModel.stages[-1]\nenron_lda = pipelineModel.transform(enron)\n\nlp_no_nums = ldaModel.logPerplexity(enron_lda)\nprint(\"Log perplexity  (w/out numbers): \",lp_no_nums)\nll_no_nums = ldaModel.logLikelihood(enron_lda)\nprint(\"Log likelihood (w/out numbers): \",ll_no_nums)\n\ntopics = ldaModel.describeTopics(6)\ntopics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\ntopics.select(\"topicWords\").show(6,truncate=False)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Removing characters from the text improves our analysis. The topics are not that informative, but better than before. What are the topics?\n- The first has to do with agreements and contracts\n- The second, third, and fourth aren't that informative. Seems like a smattering of email parts like com, subject, cc, etc.\n- The fifth seems like it has to do with management (mind, role, everyone,...)\n- The sixth topics aren't that informative either.\n\nSeems like a lot of emails have the word Monday, which suggests that employees may have been emailing over the weekend to set up in person emails to discuss the problems on Monday?"],"metadata":{}},{"cell_type":"markdown","source":["## 3\nClean the text from the body of each email message by excluding the \"quoted replies\" (i.e. the copy of the original message that is often included in a reply). How do the results of your topic model change? (Note: you might want to use RDDs and regular expressions for part of this analysis.)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\nimport re\n\nenron_data = sqlContext.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").sample(False, 0.0001)\nenron_rdd = enron_data.select(\"body\").rdd.flatMap(list)\nenron_rdd_parsed = enron_rdd.map(lambda document: document.strip().lower()).map(lambda document: re.split(\" -----Original Message-----\", str(document))[0])\nenron_df = enron_rdd_parsed.map(lambda x: Row(body= x))\nenron_df = spark.createDataFrame(enron_df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# These are same tokenizer, stop words remover, vectorizer, and lda from before.\ntokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"[a-zA-Z]*\", gaps=False)\nstopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nstopWordsRemover.loadDefaultStopWords(\"english\")\nvectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=2)\nlda = LDA(k=6, maxIter=10)\npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\npipelineModel = pipeline.fit(enron)\n\ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\nprint(\"Vocab length is\",len(cmv))\n\nldaModel = pipelineModel.stages[-1]\nenron_lda = pipelineModel.transform(enron)\nlp = ldaModel.logPerplexity(enron_lda)\nprint(\"Log perplexity: \",lp)\nll = ldaModel.logLikelihood(enron_lda)\nprint(\"Log likelihood: \",ll)\n\ntopics = ldaModel.describeTopics(8)\ntopics = topics.withColumn(\n    \"topicWords\", indices_to_terms(countVectorModel.vocabulary)(\"termIndices\"))\ntopics.select(\"topicWords\").show(10,truncate=False)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Our topic model is slightly improved. Many of the topics are still about meetings and documentation, but each topic seems more coherent. For example, the first topic has to do with an employee asking another employee about questions and changes to an agreement. Monday, April, and May all remain important topicwords. This has to do with when our data is from (in April and May, right before Enron failed as a company). There must have been a lot of changes in energy transfer pricing markets over the weekend, to explain why Monday is such a common topicword."],"metadata":{}},{"cell_type":"markdown","source":["## 4\nGiven the model from the \"best\" number of topics from Step 1, the best choice of including or excluding stopwords, and using cleaned email bodies, how consistent/stable are the topics from multiple runs (i.e. using different 1% samples)? How do you define consistency and stability?"],"metadata":{}},{"cell_type":"code","source":["iteration = []\nlog_perplexity = []\nlog_likelihood = []\n\nfor b in range(5):\n  boot = spark.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").sample(False,0.00001)\n  boot_rdd = boot.select(\"body\").rdd.flatMap(list)\n  boot_rdd_parsed = boot_rdd.map(lambda document: document.strip().lower()).map(lambda document: re.split(\" -----Original Message-----\", str(document))[0])\n  boot_df = boot_rdd_parsed.map(lambda x: Row(body= x))\n  boot_df = spark.createDataFrame(boot_df)\n  \n  tokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"[a-zA-Z]*\", gaps=False)\n  stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n  stopWordsRemover.loadDefaultStopWords(\"english\")\n  vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=2)\n  lda = LDA(k=6, maxIter=10)\n  pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, vectorizer, lda])\n  \n  pipelineModel = pipeline.fit(boot_df)\n  \n  #countVectorModel = pipelineModel.stages[-2]\n  #cmv = countVectorModel.vocabulary\n\n  ldaModel = pipelineModel.stages[-1]\n  enron_lda = pipelineModel.transform(boot_df)\n  \n  iteration.append(b)\n  lp = ldaModel.logPerplexity(enron_lda)\n  lp = round(lp, 4)\n  log_perplexity.append(lp)\n  ll = ldaModel.logLikelihood(enron_lda)\n  ll = round(ll, 4)\n  log_likelihood.append(ll)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["boot_scores = pd.DataFrame({'perplexity': log_perplexity, 'likelihood': log_likelihood}, index=boot)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["print(boot_scores.var(axis=0))\nprint(boot_scores.mean(axis=0))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["We see tiny variation in likelihood relative to the mean, but huge variation in the perplexity relative to the mean."],"metadata":{}},{"cell_type":"markdown","source":["#Above and Beyond\nFirst, calculate optimal number of topics when using bigrams instead of single tokens"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import NGram\nenron = spark.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").sample(False,0.00001)\n\nk = []\nlog_perplexity = []\nlog_likelihood = []"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["for kay in range(6,13,1):\n  print (\"k = \",kay)\n  tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n  stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n  stopWordsRemover.loadDefaultStopWords(\"english\")\n  bigram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\n  vectorizer = CountVectorizer(inputCol=\"bigrams\", outputCol=\"features\", minDF=2) \n  \n  lda = LDA(k=kay, maxIter=10)\n  pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, bigram, vectorizer, lda])\n  pipelineModel = pipeline.fit(enron)\n  \n  countVectorModel = pipelineModel.stages[-2]\n  cmv = countVectorModel.vocabulary\n\n  ldaModel = pipelineModel.stages[-1]\n  enron_lda = pipelineModel.transform(enron)\n  \n  k.append(kay)\n  lp = ldaModel.logPerplexity(enron_lda)\n  lp = round(lp, 4)\n  log_perplexity.append(lp)\n  ll = ldaModel.logLikelihood(enron_lda)\n  ll = round(ll, 4)\n  log_likelihood.append(ll)\n\ndf_lda_scores = pd.DataFrame({'perplexity': log_perplexity, 'likelihood': log_likelihood}, index=k)\ndf_lda_scores.sort_values(by=['likelihood','perplexity'], ascending=[False,True])"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Now clean the text of numbers, and get rid of quoted replies, and print\noptimal_k = 6\n\nenron_data = sqlContext.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").sample(False, 0.0001)\nenron_rdd = enron_data.select(\"body\").rdd.flatMap(list)\nenron_rdd_parsed = enron_rdd.map(lambda document: document.strip().lower()).map(lambda document: re.split(\" -----Original Message-----\", str(document))[0])\nenron_df = enron_rdd_parsed.map(lambda x: Row(body= x))\nenron_DataFrame = spark.createDataFrame(enron_df)\n\ntokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"[a-zA-Z]*\", gaps=False)\nstopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nstopWordsRemover.loadDefaultStopWords(\"english\")\nbigram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\nvectorizer = CountVectorizer(inputCol=\"bigrams\", outputCol=\"features\", minDF=2) \n  \nlda = LDA(k=optimal_k, maxIter=10)\npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, bigram, vectorizer, lda])\npipelineModel = pipeline.fit(enron_DataFrame)\n  \ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\n\nngramdf = bigram.transform(enron_DataFrame)\nngramdf.select(\"ngrams\").show(truncate=False)\n\nldaModel = pipelineModel.stages[-1]\nenron_lda = pipelineModel.transform(enron_DataFrame)\n  \nlp = ldaModel.logPerplexity(enron_lda)\nprint(round(lp, 4))\nll = ldaModel.logLikelihood(enron_lda)\nprint(round(ll, 4))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Now do the same for trigrams"],"metadata":{}},{"cell_type":"code","source":["k = []\nlog_perplexity = []\nlog_likelihood = []\n\nfor kay in range(6,13,1):\n  print (\"k = \",kay)\n  tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n  stopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n  stopWordsRemover.loadDefaultStopWords(\"english\")\n  trigram = NGram(n=3, inputCol=\"filtered\", outputCol=\"trigrams\")\n  vectorizer = CountVectorizer(inputCol=\"trigrams\", outputCol=\"features\", minDF=2) \n  \n  lda = LDA(k=kay, maxIter=10)\n  pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, trigram, vectorizer, lda])\n  pipelineModel = pipeline.fit(enron)\n  \n  countVectorModel = pipelineModel.stages[-2]\n  cmv = countVectorModel.vocabulary\n\n  ldaModel = pipelineModel.stages[-1]\n  enron_lda = pipelineModel.transform(enron)\n  \n  k.append(kay)\n  lp = ldaModel.logPerplexity(enron_lda)\n  lp = round(lp, 4)\n  log_perplexity.append(lp)\n  ll = ldaModel.logLikelihood(enron_lda)\n  ll = round(ll, 4)\n  log_likelihood.append(ll)\n\ndf_lda_scores = pd.DataFrame({'perplexity': log_perplexity, 'likelihood': log_likelihood}, index=k)\ndf_lda_scores.sort_values(by=['likelihood','perplexity'], ascending=[False,True])"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Like for bigrams, with trigrams we want 6 topics."],"metadata":{}},{"cell_type":"code","source":["# Now clean the text of numbers, and get rid of quoted replies, and print\noptimal_k = 6\n\nenron_data = sqlContext.read.parquet(\"/mnt/umsi-data-science/si618wn2017/mail.parquet\").sample(False, 0.0001)\nenron_rdd = enron_data.select(\"body\").rdd.flatMap(list)\nenron_rdd_parsed = enron_rdd.map(lambda document: document.strip().lower()).map(lambda document: re.split(\" -----Original Message-----\", str(document))[0])\nenron_df = enron_rdd_parsed.map(lambda x: Row(body= x))\nenron_DataFrame = spark.createDataFrame(enron_df)\n\ntokenizer = RegexTokenizer(inputCol=\"body\", outputCol=\"words\", pattern=\"[a-zA-Z]*\", gaps=False)\nstopWordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nstopWordsRemover.loadDefaultStopWords(\"english\")\ntrigram = NGram(n=3, inputCol=\"filtered\", outputCol=\"trigrams\")\nvectorizer = CountVectorizer(inputCol=\"trigrams\", outputCol=\"features\", minDF=2) \n  \nlda = LDA(k=optimal_k, maxIter=10)\npipeline = Pipeline(stages=[tokenizer, stopWordsRemover, trigram, vectorizer, lda])\npipelineModel = pipeline.fit(enron_DataFrame)\n  \ncountVectorModel = pipelineModel.stages[-2]\ncmv = countVectorModel.vocabulary\n\nngramdf = trigram.transform(enron_DataFrame)\nngramdf.select(\"ngrams\").show(truncate=False)\n\nldaModel = pipelineModel.stages[-1]\nenron_lda = pipelineModel.transform(enron_DataFrame)\n  \nlp = ldaModel.logPerplexity(enron_lda)\nprint(round(lp, 4))\nll = ldaModel.logLikelihood(enron_lda)\nprint(round(ll, 4))"],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"Homework 8 - Improving LDA","notebookId":676848791300095},"nbformat":4,"nbformat_minor":0}
