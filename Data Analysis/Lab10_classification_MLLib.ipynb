{"cells":[{"cell_type":"markdown","source":["# SI 618 - Lab 10 - Classification using MLLib\n\n## Objectives\n\n- Be able to perform classifications and regressions using the following methods and interpret their results.\n  - Naive Bayes (classification)\n  - Random Forest (classification & regression) \n- Understand why and how to split the data into training and testing set.\n- Know how to choose and rank features using Random Forest's feature importance measure.\n\n## Submission Instructions:\nPlease turn in your completed Databricks notebook in .html format as well as the link to the published version of if via Canvas."],"metadata":{}},{"cell_type":"code","source":["#import pandas as pd\nimport numpy as np\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nimport seaborn as sns\nimport matplotlib.pyplot as plt"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["ACCESS_KEY = \nSECRET_KEY = \nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"umsi-data-science-west\"\nMOUNT_NAME = \"umsi-data-science\"\ntry:\n  dbutils.fs.unmount(\"/mnt/%s/\" % MOUNT_NAME)\nexcept:\n  print(\"Could not unmount %s, but that's ok.\" % MOUNT_NAME)\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\ndisplay(dbutils.fs.ls(\"/mnt/umsi-data-science/si618wn2017\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## 1. Naive Bayes Classifier\n\n### 1.1 Example code"],"metadata":{}},{"cell_type":"markdown","source":["### Goal: \nWe will show you an example of how to train a naive Bayes classifier to classify iris species. Here are the specifications:\n- __Objective__: predict which species an iris instance belongs to.\n- __Possible classes__: \"setosa\", \"versicolor\", and \"virginica\"\n- __Features__: all four features: sepal_length, sepal_width, petal_length, petal_width"],"metadata":{}},{"cell_type":"markdown","source":["#### Data Summary:\nThe iris data set contains 3 classes (setosa, versicolor, and virginica) of 50 instances each, where each class refers to a type of iris plant.\n\nThe data is downloaded from UCI Machine Learning Repository. For more information, refer to:\nhttps://archive.ics.uci.edu/ml/datasets/iris"],"metadata":{}},{"cell_type":"markdown","source":["#### Load and show the iris dataset."],"metadata":{}},{"cell_type":"markdown","source":["**NOTE** Need to explain what we're doing with sns here: The Iris data is most easily obtained by loaded the Seaborn package, which is within the package."],"metadata":{}},{"cell_type":"code","source":["import seaborn as sns\ndf_iris = sns.load_dataset('iris')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["df_iris.head(5)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["pandas_df_iris = sns.load_dataset('iris')\ndf_iris = sqlContext.createDataFrame(pandas_df_iris)\ndf_iris.show(5)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["#### Train and test a Naive Bayes classifier on iris data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Split the data into train and test, let's try a 60/40 split and set a random seed so we can reproduce the results\nsplits = df_iris.randomSplit([0.6, 0.4], 1234)\n\n# Training gets the 60%\niris_train = splits[0]\n\n# Testing gets the 40%\niris_test = splits[1]\n\n# encodes the column of species to a column of species indices\niris_string_indexer = StringIndexer(inputCol=\"species\", outputCol=\"indexed_species\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["iris_string_indexer"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# merges multiple columns into a vector column\niris_assembler = VectorAssembler(\n    inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\n    outputCol=\"features\"\n)\n\n# create the trainer and set its parameters; set smoothing=10.0, which represents add-one smoothing\niris_nb = NaiveBayes(featuresCol='features', labelCol='indexed_species', smoothing=1.0, modelType=\"multinomial\")\n\n# maps the column of prediction indices back to the original species labels -- WHY?\niris_labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=iris_string_indexer.fit(df_iris).labels)\n\n# Chain indexers and forest in a Pipeline\niris_pipeline = Pipeline(stages=[iris_string_indexer, iris_assembler, iris_nb, iris_labelConverter])\n\n# Train the pipeline on the train dataset\niris_model = iris_pipeline.fit(iris_train)\n\n# select example rows to display.\niris_predictions = iris_model.transform(iris_test)\n\n# compute accuracy on the test set\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_species\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(iris_predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["display(iris_predictions.select(\"prediction\",\"indexed_species\"))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### 1.2 Do it yourself:  Use a Naive Bayes Classifier to identify spam email messages"],"metadata":{}},{"cell_type":"markdown","source":["### Goal:\nUse the example above (1.1) as the basis for your work.   We will train a Naive Bayes classifier to detect e-mail spam. Here are the specifications:\n- __Objective__: predict whether each e-mail is spam or not.\n- __Possible classes__: spam: 1, non-spam: 0\n- __Features__: all 57 features"],"metadata":{}},{"cell_type":"markdown","source":["#### Data Summary:\nThe last column of the spam dataset denotes whether the e-mail was \nconsidered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \nMost of the attributes indicate whether a particular word or\ncharacter was frequently occuring in the e-mail.\n\nThe data is downloaded from UCI Machine Learning Repository. For more information, refer to:\nhttps://archive.ics.uci.edu/ml/datasets/spambase"],"metadata":{}},{"cell_type":"markdown","source":["#### Load and show the spam dataset."],"metadata":{}},{"cell_type":"code","source":["# a bunch of columns of data, no header\nspam = spark.read.csv(\"/mnt/umsi-data-science/si618wn2017/spam.csv\", inferSchema=True)\n# variable names are stored in another csv file\nspam_variable_name = spark.read.csv(\"/mnt/umsi-data-science/si618wn2017/spam_variable_name.csv\")\n# this puts the data together\nfor i in np.arange(len(spam.columns)):\n  spam = spam.withColumnRenamed(spam.columns[i], spam_variable_name.toPandas().iloc[i][0].strip())\ndisplay(spam)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Step 1: Split the data using a 70%-30% training-testing split, and set the seed number of the random number generator to be 1."],"metadata":{}},{"cell_type":"code","source":["splits = spam.randomSplit([0.7, 0.3], 1)\nspam_train = splits[0]\nspam_test = splits[1]"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Step 2: Combine all 57 features into one column named \"features\" using VectorAssembler(), excluding the target column \"spam_or_not\"."],"metadata":{}},{"cell_type":"code","source":["cols = spam.columns[:-1]\nspam_assembler = VectorAssembler(\n    inputCols=cols,\n    outputCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Step 3: Create a NaiveBayes classifier with add-one smoothing."],"metadata":{}},{"cell_type":"code","source":["spam_nb = NaiveBayes(featuresCol='features', labelCol='spam_or_not', smoothing=1.0, modelType=\"multinomial\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### Step 4: Create a pipeline that includes assembler and nb defined above in Step 2 - Step 3."],"metadata":{}},{"cell_type":"code","source":["spam_pipeline = Pipeline(stages=[spam_assembler, spam_nb])"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Step 5: Train the pipeline object on the training set, and make predictions on the testing set."],"metadata":{}},{"cell_type":"code","source":["spam_model = spam_pipeline.fit(spam_train)\nspam_predictions = spam_model.transform(spam_test)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Step 6: Display the predictions and the true spam labels."],"metadata":{}},{"cell_type":"code","source":["display(spam_predictions.select(\"prediction\",\"spam_or_not\"))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Step 7: Evaluate how good the spam detection classifier is."],"metadata":{}},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"spam_or_not\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(spam_predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["## 2. Random Forest\n\n### 2.1 Example code:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["#### Let's use the iris data again as an example."],"metadata":{}},{"cell_type":"code","source":["df_iris.show(5)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["#### Train and test a Random Forest classifier on iris data."],"metadata":{}},{"cell_type":"code","source":["# create the trainer and set its parameters; set smoothing=10.0, which represents add-one smoothing\niris_rf = RandomForestClassifier(labelCol=\"indexed_species\", featuresCol=\"features\", numTrees=10)\n\n# Chain indexers and forest in a Pipeline\niris_pipeline = Pipeline(stages=[iris_string_indexer, iris_assembler, iris_rf, iris_labelConverter])\n\n# Train the pipeline on the train dataset\niris_model = iris_pipeline.fit(iris_train)\n\n# select example rows to display.\niris_predictions = iris_model.transform(iris_test)\n\n# compute accuracy on the test set\nevaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_species\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(iris_predictions)\nprint(\"Test set accuracy = \" + str(accuracy))\n\n# compute feature importance of each predictor\nfeatureImportances = pd.DataFrame({\"index\":df_iris.columns[:-1],\"featureImportances\":iris_model.stages[2].featureImportances})\\\n  .sort_values(\"featureImportances\", ascending=False)\n\n# visualize the feature importance using seaborn\nf, ax = plt.subplots(figsize=(12, 7))\nsns.set(style=\"darkgrid\")\nsns.barplot(y=\"index\",x=\"featureImportances\",data=featureImportances)\n\ndisplay(f.figure)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["### 2.2 Do it yourself!\n\n### Goal:\nModel After the above example code. Try applying a Decision Tree classifier to detect the spam e-mails."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Create a DecisionTree classifier."],"metadata":{}},{"cell_type":"code","source":["spam_rf = RandomForestClassifier(labelCol=\"spam_or_not\", featuresCol=\"features\", numTrees=10)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["### Step 2: Create a pipeline that includes spam_assembler and spam_rf defined above."],"metadata":{}},{"cell_type":"code","source":["spam_pipeline = Pipeline(stages=[spam_assembler, spam_rf])"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### Step 3: Train the pipeline object on the training set, and make predictions on the testing set."],"metadata":{}},{"cell_type":"code","source":["spam_model = spam_pipeline.fit(spam_train)\nspam_predictions = spam_model.transform(spam_test)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["### Step 4: Display the predictions and the true spam labels."],"metadata":{}},{"cell_type":"code","source":["# You might need to change the following line\ndisplay(spam_predictions.select(\"prediction\",\"spam_or_not\"))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### Step 5: Evaluate how good the spam detection classifier is."],"metadata":{}},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"spam_or_not\", predictionCol=\"prediction\",\n                                              metricName=\"accuracy\")\naccuracy = evaluator.evaluate(spam_predictions)\nprint(\"Test set accuracy = \" + str(accuracy))"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["### Step 6: List every predictor and their feature importance in descending order."],"metadata":{}},{"cell_type":"code","source":["featureImportances = pd.DataFrame({\"index\":spam.columns[:-1],\"featureImportances\":spam_model.stages[1].featureImportances})\\\n  .sort_values(\"featureImportances\", ascending=False)\nfeatureImportances.head(20)  "],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["### Step 7: Visualize the feature importance using Seaborn. Briefly interpret the plot and describe what you think are the important factors that determine if an e-mail is spam or not."],"metadata":{}},{"cell_type":"markdown","source":["#### For your reference, here is some of the data description:\n|Feature|Description|\n|--|--|\n|word_freq_WORD|percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) total number of words in e-mail.  A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.|\n|char_freq_CHAR|percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail|\n|capital_run_length_average|average length of uninterrupted sequences of capital letters|\n|capital_run_length_longest|length of longest uninterrupted sequence of capital letters|\n|capital_run_length_total|total number of capital letters in the e-mail |"],"metadata":{}},{"cell_type":"code","source":["# visualize the feature importance using seaborn\nf, ax = plt.subplots(figsize=(12, 14))\nsns.set(style=\"darkgrid\")\nsns.barplot(y=\"index\",x=\"featureImportances\", data=featureImportances)\n\ndisplay(f.figure)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["The most important factors that predict whether an email message is spam are the frequency of characters not common to how people speak, like $ and !, and words like free, money, and credit. This makes sense, as most spam messages are trying to sell you something."],"metadata":{}},{"cell_type":"markdown","source":["## End of Lab 10"],"metadata":{}}],"metadata":{"name":"SI 618 WN 2018 Lab 10 - Classification using MLLib","notebookId":1642819286896623},"nbformat":4,"nbformat_minor":0}
