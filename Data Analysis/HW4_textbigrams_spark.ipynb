{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 618 WN 2018 - Homework 4: Using the Spark RDD API to analyze bigrams in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "1. To gain familiarity with PySpark\n",
    "2. To learn the basics of the Spark RDD API\n",
    "3. To get experience finding and downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please fill in...\n",
    "### * Your name: Anthony Cozart\n",
    "### * People you worked with: Anna Lenhart, Lauren (@ Office Hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions:\n",
    "Please turn in this Jupyter notebook file (in both .ipynb and .html formats) **and the text file that contains the text you analyzed** via Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project is designed to give you a basic familiarity with the Apache Spark RDD API.\n",
    "\n",
    "**Your first task** is to run the next code cell in this notebook, as is, (without modification) and confirm that everything is working.\n",
    "\n",
    "Your second and main task is to modify the word_count_v2.ipynb file in Lab 4 to create a si618-hw4-YOUR_UNIQNAME, which counts the number of bigrams within the corpus. At the conclusion of its execution, it should output three pieces of information: \n",
    "1. the total number of bigrams\n",
    "2. the 20 most common bigrams\n",
    "3. the minimum number of bigrams required to add up to 10% of all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11),\n",
       " ('was', 10),\n",
       " ('of', 10),\n",
       " ('it', 9),\n",
       " ('we', 4),\n",
       " ('epoch', 2),\n",
       " ('before', 2),\n",
       " ('us', 2),\n",
       " ('times', 2),\n",
       " ('age', 2)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "totc = [\"It was the best of times\",\n",
    "    \"it was the worst of times\",\n",
    "    \"it was the age of wisdom\",\n",
    "    \"it was the age of foolishness\",\n",
    "    \"it was the epoch of belief\",\n",
    "    \"it was the epoch of incredulity\",\n",
    "    \"it was the season of Light\",\n",
    "    \"it was the season of Darkness\",\n",
    "    \"it was the spring of hope\",\n",
    "    \"it was the winter of despair\",\n",
    "    \"we had everything before us\",\n",
    "    \"we had nothing before us\",\n",
    "    \"we were all going direct to Heaven\",\n",
    "    \"we were all going direct the other way\"]\n",
    "\n",
    "WORD_RE = re.compile(r\"\\b[\\w']+\\b\")\n",
    "\n",
    "input_text = sc.parallelize(totc)\n",
    "word_counts_sorted = input_text.flatMap(lambda line: WORD_RE.findall(line)) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda accumulator, value: accumulator + value) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "top10_sorted = word_counts_sorted.take(10)\n",
    "top10_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Analysis\n",
    "Bigrams are pairs of continguous words.  For example, consider the text \"The quick brown fox\".  The bigrams in that text are: (The, quick), (quick, brown), (brown,fox).\n",
    "\n",
    "You should be able to use the above code as a starting point, with the main difference being that you will be extracting pairs of words rather than single words.\n",
    "\n",
    "Here are the steps you need to include:\n",
    "\n",
    "1. Find and download a text file from the Gutenberg Project.  Please select a book written in English and download the plain text version (i.e. the .txt file).  \n",
    "1. Normalize the text by converting it to lowercase.  \n",
    "1. Extract all bigrams from the text.  Donâ€™t try to get fancy: just take all the pairs of words from each line of your text file\n",
    "1. Perform a mapping to yield a count of one for each bigram (e.g. ((\"the\", \"quick\"), 1)). \n",
    "1. Reduce this list of bigrams with count of one to counts of bigrams (e.g. ((\"the\", \"quick\"), 312)).\n",
    "1. Sort to determine the most frequent and total number of bigrams. \n",
    "1. Report the total number of bigrams, the top 20 most common bigrams and the minimum number of bigrams required to add up to 10% of all bigrams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Find and download a text file\n",
    "Go to Project Gutenberg (http://www.gutenberg.org) and find a book written in English that you think sounds interesting. \n",
    "Download the plain text (.txt) version of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1b: Load the text file into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file_name = \"war_peace_tolstoy.txt\"\n",
    "input_text = sc.textFile(text_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the project gutenberg ebook of war and peace, by leo tolstoy\n",
      "\n",
      "this ebook is for the use of anyone anywhere at no cost and with almost\n",
      "no restrictions whatsoever. you may copy it, give it away or re-use\n",
      "it under the terms of the project gutenberg license included with this\n",
      "ebook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "title: war and peace\n"
     ]
    }
   ],
   "source": [
    "for line in input_text.take(10):\n",
    "    print(line.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 2 and 3: Normalize text and extract bigrams\n",
    "Note: there are many ways to accomplish this.  We recommend you create\n",
    "a function that both creates bigrams and normalizes text.  The following\n",
    "template code assumes this is the approach you are planning to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = input_text.map(lambda line: line.lower().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Anna's function here -- the only (but key) difference from mine is that she saves the bigram separately and then together, instead of \"returning\" each\n",
    "def normalize_and_extract_bigrams(words):\n",
    "    bigrams=[]\n",
    "    print(words)\n",
    "    for i in range (0, len(words)-1):\n",
    "        bigram=(words[i], words[i+1])\n",
    "        #bigram.lower\n",
    "        bigrams.append(bigram)       \n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams1 = words.flatMap(normalize_and_extract_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Map the bigrams to key-value pairs where the key is the bigram and the value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams2 = bigrams1.map(lambda words: (words, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Reduce the (word,1) key-value pairs to give you counts of each bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams3 = bigrams2.reduceByKey(lambda accumulator, value: accumulator + value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sort the resulting RDD by value in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigrams_sorted = bigrams3.sortBy(lambda x: x[1], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Report the required values\n",
    "1. the total number of bigrams\n",
    "2. the 20 most common bigrams\n",
    "3. the minimum number of bigrams required to add up to 10% of all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517022"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. The total number of bigrams (this is not the number of bigrams, but the count -- all occurences)\n",
    "total_bigrams = bigrams_sorted.values().sum()\n",
    "total_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 3851),\n",
       " (('to', 'the'), 2189),\n",
       " (('', ''), 2182),\n",
       " (('in', 'the'), 2174),\n",
       " (('and', 'the'), 1390),\n",
       " (('at', 'the'), 1281),\n",
       " (('on', 'the'), 1236),\n",
       " (('he', 'had'), 1141),\n",
       " (('did', 'not'), 994),\n",
       " (('with', 'a'), 898),\n",
       " (('he', 'was'), 858),\n",
       " (('from', 'the'), 815),\n",
       " (('it', 'was'), 797),\n",
       " (('with', 'the'), 755),\n",
       " (('of', 'his'), 743),\n",
       " (('by', 'the'), 733),\n",
       " (('in', 'a'), 704),\n",
       " (('to', 'be'), 698),\n",
       " (('had', 'been'), 695),\n",
       " (('prince', 'andrew'), 630)]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. The 20 most common bigrams\n",
    "top20 = bigrams_sorted.take(20)\n",
    "top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 bigrams comprise 10% of the total number of bigrams in War & Peace.\n"
     ]
    }
   ],
   "source": [
    "# 3. the minimum number of bigrams required to add up to 10% of all bigrams\n",
    "def running_total(a):\n",
    "    threshold = total_bigrams/10\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for i in a:\n",
    "        if total < threshold:\n",
    "            total += i[1]\n",
    "            count += 1\n",
    "        else:\n",
    "            return count\n",
    "\n",
    "print(\"{0:d} bigrams comprise 10% of the total number of bigrams in War & Peace.\".format(running_total(bigrams_sorted.take(500))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above and Beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anna Karenina, War & Peace, and the Brothers Karamazov are among the most highly regarded novels in the world. All three are by russian novelists, and are very very long. \n",
    "\n",
    "After looking at the counts of bigrams of for War & Peace, I became interested in the concentration of words in novels. So I looked up the other two novels, ran word counts, and used a measure of concentration from public policy and economics called a Herfindahl Index (https://en.wikipedia.org/wiki/Herfindahl_index).\n",
    "\n",
    "The Herfindahl Index is the square of the count divided by the total number of words. It's typically used to understand the market power (control) of firms -- for example, how much power does Verizon have compared to AT&T. To calculate it, I take the top 1000 words in each book, to speed up computing.\n",
    "\n",
    "The concentration of words is similar for all three books -- maybe that's not so suprising, since all three are written in a similar, complex style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ak_txt = \"anna_karenina_tolstoy.txt\"\n",
    "input_text = sc.textFile(ak_txt)\n",
    "\n",
    "ak_word_counts = input_text.flatMap(lambda line: WORD_RE.findall(line)) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda accumulator, value: accumulator + value) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "ak_top1000 = ak_word_counts.take(1000)\n",
    "ak_total = ak_word_counts.values().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wp_txt = \"war_peace_tolstoy.txt\"\n",
    "input_text = sc.textFile(wp_txt)\n",
    "\n",
    "wp_word_counts = input_text.flatMap(lambda line: WORD_RE.findall(line)) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda accumulator, value: accumulator + value) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "wp_top1000 = wp_word_counts.take(1000)\n",
    "wp_total = wp_word_counts.values().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk_txt = \"brothers_karamazov.txt\"\n",
    "input_text = sc.textFile(bk_txt)\n",
    "\n",
    "bk_word_counts = input_text.flatMap(lambda line: WORD_RE.findall(line)) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda accumulator, value: accumulator + value) \\\n",
    "    .sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "bk_top1000 = bk_word_counts.take(1000)\n",
    "bk_total = bk_word_counts.values().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ak = pd.DataFrame(list(ak_top1000), columns =['word', 'count'])\n",
    "wp = pd.DataFrame(list(wp_top1000), columns =['word', 'count'])\n",
    "bk = pd.DataFrame(list(bk_top1000), columns =['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp['share'] = (wp['count']^2)/int(wp_total)\n",
    "ak['share'] = (ak['count']^2)/int(ak_total)\n",
    "bk['share'] = (bk['count']^2)/int(bk_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a similar spirit to calculating bigrams, we can see just how concentrated the words are in each novel:\n",
      "The Herfindahl index for Anna Karenina is 0.828109.\n",
      "The Herfindahl index for War & Peace is 0.796827.\n",
      "The Herfindahl index for Brothers Karamazov is 0.832341.\n"
     ]
    }
   ],
   "source": [
    "print(\"In a similar spirit to calculating bigrams, we can see just how concentrated the words are in each novel:\")\n",
    "print(\"The Herfindahl index for Anna Karenina is {0:f}.\".format(ak['share'].sum()))\n",
    "print(\"The Herfindahl index for War & Peace is {0:f}.\".format(wp['share'].sum()))\n",
    "print(\"The Herfindahl index for Brothers Karamazov is {0:f}.\".format(bk['share'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361904\n",
      "576627\n",
      "363547\n"
     ]
    }
   ],
   "source": [
    "print(ak_total)\n",
    "print(wp_total)\n",
    "print(bk_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF HOMEWORK 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
